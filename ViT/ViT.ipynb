{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc94b56-ba58-4bb8-b7c7-ca278d87a8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f900488c3f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df399c-b2d9-4281-8695-1a9dc30d9b71",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b313ae7-8fda-4455-b303-4c1fa9058e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./../datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 32277950.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./../datasets/MNIST/raw/train-images-idx3-ubyte.gz to ./../datasets/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./../datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 68983880.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./../datasets/MNIST/raw/train-labels-idx1-ubyte.gz to ./../datasets/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./../datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 26676533.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./../datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to ./../datasets/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./../datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 20159289.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./../datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./../datasets/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "transform = ToTensor()\n",
    "\n",
    "train_set = MNIST(root='./../datasets', train=True, download=True, transform=transform)\n",
    "test_set = MNIST(root='./../datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a36e7f-d8c9-4a3b-b707-00d35a0b7183",
   "metadata": {},
   "source": [
    "### the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df154e0-2b68-471d-a62b-f58aa703e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_loader, test_loader):\n",
    "\n",
    "    # Defining model and training options\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "    model = MyViT((1, 28, 28), n_patches=7, n_blocks=2, embed_dim = 32, hidden_d=8, n_heads=2, out_d=10).to(device)\n",
    "    N_EPOCHS = 10\n",
    "    LR = 0.005\n",
    "\n",
    "    # Training loop\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "    criterion = CrossEntropyLoss()\n",
    "    for epoch in trange(N_EPOCHS, desc=\"Training\"):\n",
    "        train_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "\n",
    "            train_loss += loss.detach().cpu().item() / len(train_loader)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")\n",
    "\n",
    "    # Test loop\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        test_loss = 0.0\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "\n",
    "            correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
    "            total += len(x)\n",
    "        print(f\"Test loss: {test_loss:.2f}\")\n",
    "        print(f\"Test accuracy: {correct / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe54c17a-f23f-4be5-8b01-2d238698d21d",
   "metadata": {},
   "source": [
    "### This is only the first part of the ViT, containing only convolutional projection and tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b53d417-5eec-44cd-ad41-98142d1ffe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMiniViT(nn.Module):\n",
    "  def __init__(self, chw=(1, 28, 28), n_patches=7, embed_dim = 32, hidden_d = 8):\n",
    "    # Super constructor\n",
    "    super(MyMiniViT, self).__init__()\n",
    "\n",
    "    # Attributes\n",
    "    self.chw = chw # (C, H, W)\n",
    "    self.n_patches = n_patches\n",
    "\n",
    "    assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "    assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "    self.patch_size = (chw[1] // n_patches, chw[2] // n_patches)\n",
    "\n",
    "    # 1) Projector and Linear mapper (perhaps we can get by without the Linear mapper?)\n",
    "    in_chans = chw[0]\n",
    "    self.proj = nn.Conv2d(in_chans, embed_dim,\n",
    "                              kernel_size=self.patch_size, stride=self.patch_size)\n",
    "    self.linear_mapper = nn.Linear(embed_dim, hidden_d)\n",
    "\n",
    "    # 2) Learnable classifiation token\n",
    "    self.class_token = nn.Parameter(torch.rand(1, hidden_d))\n",
    "\n",
    "  def forward(self, images):\n",
    "\n",
    "    patches = self.proj(images).flatten(2).transpose(1, 2)#.to(self.positional_embeddings.device)\n",
    "    tokens = self.linear_mapper(patches)\n",
    "\n",
    "    # Adding classification token to the tokens\n",
    "    tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2f4755-4429-4439-97ee-e96446c97ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "675dfe87-f149-4b49-b668-d4b712283fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(get_positional_embeddings(50, 32), cmap=\"hot\", interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fc6f7b5-c3f9-40fc-8aa9-4970f1c1958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is only the first part of the ViT plus positional embeddings\n",
    "class MyMidiViT(nn.Module):\n",
    "  def __init__(self, chw=(1, 28, 28), n_patches=7, embed_dim = 32, hidden_d = 8):\n",
    "    # Super constructor\n",
    "    super(MyMidiViT, self).__init__()\n",
    "\n",
    "    # Attributes\n",
    "    self.chw = chw # (C, H, W)\n",
    "    self.n_patches = n_patches\n",
    "\n",
    "    assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "    assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "    self.patch_size = (chw[1] // n_patches, chw[2] // n_patches)\n",
    "\n",
    "    # 1) Projector and Linear mapper (perhaps we can get by without the Linear mapper?)\n",
    "    in_chans = chw[0]\n",
    "    self.proj = nn.Conv2d(in_chans, embed_dim,\n",
    "                              kernel_size=self.patch_size, stride=self.patch_size)\n",
    "    self.linear_mapper = nn.Linear(embed_dim, hidden_d)\n",
    "\n",
    "    # 2) Learnable classifiation token\n",
    "    self.class_token = nn.Parameter(torch.rand(1, hidden_d))\n",
    "\n",
    "    # 3) Positional embedding\n",
    "    # n_patches ** 2 + 1 is the number of tokens: one per patch (n_patches^2)  plus learnable token\n",
    "    self.pos_embed = nn.Parameter(torch.tensor(get_positional_embeddings(n_patches ** 2 + 1, hidden_d)))\n",
    "    self.pos_embed.requires_grad = False\n",
    "\n",
    "\n",
    "  def forward(self, images):\n",
    "    n, c, h, w = images.shape\n",
    "    patches = self.proj(images).flatten(2).transpose(1, 2)#.to(self.positional_embeddings.device)\n",
    "    tokens = self.linear_mapper(patches)\n",
    "\n",
    "    # Adding classification token to the tokens\n",
    "    tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n",
    "\n",
    "    # Adding positional embedding\n",
    "    pos_embed = self.pos_embed.repeat(n, 1, 1)\n",
    "    out = tokens + pos_embed\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b344cc93-1749-40fe-a4c1-bf04251aaf19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_image,_ = test_set[0]\n",
    "tst_image = tst_image.unsqueeze(0)\n",
    "tst_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cda28d5-56a8-4a0c-b52b-e047dfec7b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26019/3107864954.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.pos_embed = nn.Parameter(torch.tensor(get_positional_embeddings(n_patches ** 2 + 1, hidden_d)))\n"
     ]
    }
   ],
   "source": [
    "mini = MyMiniViT()\n",
    "midi = MyMidiViT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ded9635b-7fc1-47e8-beb5-fec11f27035f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens=mini(tst_image)\n",
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fcdc881-4e14-4f2b-92c4-8d71dff65a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is multi-head self-attention submodule\n",
    "\n",
    "class MyMSA(nn.Module):\n",
    "    def __init__(self, d, n_heads=2):\n",
    "        super(MyMSA, self).__init__()\n",
    "        self.d = d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
    "\n",
    "        d_head = int(d / n_heads)\n",
    "        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.d_head = d_head\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        # Sequences has shape (N, seq_length, token_dim)\n",
    "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "        result = []\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            #import ipdb; ipdb.set_trace()\n",
    "            for head in range(self.n_heads):\n",
    "                q_mapping = self.q_mappings[head]\n",
    "                k_mapping = self.k_mappings[head]\n",
    "                v_mapping = self.v_mappings[head]\n",
    "\n",
    "                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "                seq_result.append(attention @ v)\n",
    "            result.append(torch.hstack(seq_result))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n",
    "\n",
    "\n",
    "# This is multi-head self-attention submodule packaged in  block with a residual connection\n",
    "\n",
    "class MyViTBlock(nn.Module):\n",
    "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
    "        super(MyViTBlock, self).__init__()\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_d)\n",
    "        self.mhsa = MyMSA(hidden_d, n_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.mhsa(self.norm1(x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f830d1af-6c8a-4e91-ac04-2260b44c9d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete ViT\n",
    "\n",
    "class MyViT(nn.Module):\n",
    "    def __init__(self, chw, n_patches=7, n_blocks=2, embed_dim = 32, hidden_d=8, n_heads=2, out_d=10):\n",
    "        # Super constructor\n",
    "        super(MyViT, self).__init__()\n",
    "        \n",
    "        # Attributes\n",
    "        self.chw = chw # ( C , H , W )\n",
    "        self.n_patches = n_patches\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_d = hidden_d\n",
    "        \n",
    "        # Input and patches sizes\n",
    "        assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        self.patch_size = (chw[1] // n_patches, chw[2] // n_patches)\n",
    "\n",
    "        # 1) Projector and Linear mapper (perhaps we can get by without the Linear mapper?)\n",
    "        in_chans = chw[0]\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim,\n",
    "                              kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        self.linear_mapper = nn.Linear(embed_dim, hidden_d)\n",
    "\n",
    "        \n",
    "        # 2) Learnable classification token\n",
    "        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "        \n",
    "        # 3) Positional embedding\n",
    "        self.register_buffer('positional_embeddings', get_positional_embeddings(n_patches ** 2 + 1, hidden_d), persistent=False)\n",
    "        \n",
    "        # 4) Transformer encoder blocks\n",
    "        self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n",
    "        \n",
    "        # 5) Classification MLPk\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.hidden_d, out_d),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Dividing images into patches\n",
    "        n, c, h, w = images.shape\n",
    "        patches = self.proj(images).flatten(2).transpose(1, 2).to(self.positional_embeddings.device)\n",
    "        tokens = self.linear_mapper(patches)\n",
    "\n",
    "        # Adding classification token to the tokens\n",
    "        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
    "        \n",
    "        # Adding positional embedding\n",
    "        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
    "        \n",
    "        # Transformer Blocks\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "            \n",
    "        # Getting the classification token only\n",
    "        out = out[:, 0]\n",
    "        \n",
    "        return self.mlp(out) # Map to output dimension, output category distribution\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce8af8bd-bdc3-47ca-a76e-e0d33405c570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxi = MyViT((1, 28, 28), n_patches=7, n_blocks=2, embed_dim = 32, hidden_d=8, n_heads=2, out_d=10)\n",
    "out=maxi(tst_image)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "112061a6-f839-42e9-a05f-7049348ed7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda:1 (NVIDIA GeForce RTX 4090)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]/home/woreom/miniconda3/envs/adv/lib/python3.9/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1702400441250/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "Training:  10%|█         | 1/10 [01:51<16:46, 111.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 loss: 2.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 2/10 [03:31<13:55, 104.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 loss: 1.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 3/10 [05:06<11:42, 100.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 loss: 1.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 4/10 [06:41<09:48, 98.16s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 loss: 1.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 5/10 [08:24<08:20, 100.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 loss: 1.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 6/10 [10:13<06:51, 102.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 loss: 1.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 7/10 [11:50<05:03, 101.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 loss: 1.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 8/10 [13:28<03:19, 99.99s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 loss: 1.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 9/10 [15:02<01:38, 98.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 loss: 1.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [16:53<00:00, 101.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 loss: 1.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 79/79 [00:04<00:00, 19.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.80\n",
      "Test accuracy: 66.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main(train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21c1d5f-251c-4d02-8ec7-a27faf88f0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
