{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc94b56-ba58-4bb8-b7c7-ca278d87a8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df399c-b2d9-4281-8695-1a9dc30d9b71",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b313ae7-8fda-4455-b303-4c1fa9058e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "transform = ToTensor()\n",
    "\n",
    "train_set = MNIST(root='./../datasets', train=True, download=True, transform=transform)\n",
    "test_set = MNIST(root='./../datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a36e7f-d8c9-4a3b-b707-00d35a0b7183",
   "metadata": {},
   "source": [
    "### the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df154e0-2b68-471d-a62b-f58aa703e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_loader, test_loader):\n",
    "\n",
    "    # Defining model and training options\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "    model = MyViT((1, 28, 28), n_patches=7, n_blocks=2, embed_dim = 32, hidden_d=8, n_heads=2, out_d=10).to(device)\n",
    "    N_EPOCHS = 10\n",
    "    LR = 0.005\n",
    "\n",
    "    # Training loop\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "    criterion = CrossEntropyLoss()\n",
    "    for epoch in trange(N_EPOCHS, desc=\"Training\"):\n",
    "        train_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "\n",
    "            train_loss += loss.detach().cpu().item() / len(train_loader)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")\n",
    "\n",
    "    # Test loop\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        test_loss = 0.0\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "\n",
    "            correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
    "            total += len(x)\n",
    "        print(f\"Test loss: {test_loss:.2f}\")\n",
    "        print(f\"Test accuracy: {correct / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe54c17a-f23f-4be5-8b01-2d238698d21d",
   "metadata": {},
   "source": [
    "### This is only the first part of the ViT, containing only convolutional projection and tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b53d417-5eec-44cd-ad41-98142d1ffe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMiniViT(nn.Module):\n",
    "  def __init__(self, chw=(1, 28, 28), n_patches=7, embed_dim = 32, hidden_d = 8):\n",
    "    # Super constructor\n",
    "    super(MyMiniViT, self).__init__()\n",
    "\n",
    "    # Attributes\n",
    "    self.chw = chw # (C, H, W)\n",
    "    self.n_patches = n_patches\n",
    "\n",
    "    assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "    assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "    self.patch_size = (chw[1] // n_patches, chw[2] // n_patches)\n",
    "\n",
    "    # 1) Projector and Linear mapper (perhaps we can get by without the Linear mapper?)\n",
    "    in_chans = chw[0]\n",
    "    self.proj = nn.Conv2d(in_chans, embed_dim,\n",
    "                              kernel_size=self.patch_size, stride=self.patch_size)\n",
    "    self.linear_mapper = nn.Linear(embed_dim, hidden_d)\n",
    "\n",
    "    # 2) Learnable classifiation token\n",
    "    self.class_token = nn.Parameter(torch.rand(1, hidden_d))\n",
    "\n",
    "  def forward(self, images):\n",
    "\n",
    "    patches = self.proj(images).flatten(2).transpose(1, 2)#.to(self.positional_embeddings.device)\n",
    "    tokens = self.linear_mapper(patches)\n",
    "\n",
    "    # Adding classification token to the tokens\n",
    "    tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f4755-4429-4439-97ee-e96446c97ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675dfe87-f149-4b49-b668-d4b712283fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(get_positional_embeddings(50, 32), cmap=\"hot\", interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc6f7b5-c3f9-40fc-8aa9-4970f1c1958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is only the first part of the ViT plus positional embeddings\n",
    "class MyMidiViT(nn.Module):\n",
    "  def __init__(self, chw=(1, 28, 28), n_patches=7, embed_dim = 32, hidden_d = 8):\n",
    "    # Super constructor\n",
    "    super(MyMidiViT, self).__init__()\n",
    "\n",
    "    # Attributes\n",
    "    self.chw = chw # (C, H, W)\n",
    "    self.n_patches = n_patches\n",
    "\n",
    "    assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "    assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "    self.patch_size = (chw[1] // n_patches, chw[2] // n_patches)\n",
    "\n",
    "    # 1) Projector and Linear mapper (perhaps we can get by without the Linear mapper?)\n",
    "    in_chans = chw[0]\n",
    "    self.proj = nn.Conv2d(in_chans, embed_dim,\n",
    "                              kernel_size=self.patch_size, stride=self.patch_size)\n",
    "    self.linear_mapper = nn.Linear(embed_dim, hidden_d)\n",
    "\n",
    "    # 2) Learnable classifiation token\n",
    "    self.class_token = nn.Parameter(torch.rand(1, hidden_d))\n",
    "\n",
    "    # 3) Positional embedding\n",
    "    # n_patches ** 2 + 1 is the number of tokens: one per patch (n_patches^2)  plus learnable token\n",
    "    self.pos_embed = nn.Parameter(torch.tensor(get_positional_embeddings(n_patches ** 2 + 1, hidden_d)))\n",
    "    self.pos_embed.requires_grad = False\n",
    "\n",
    "\n",
    "  def forward(self, images):\n",
    "    n, c, h, w = images.shape\n",
    "    patches = self.proj(images).flatten(2).transpose(1, 2)#.to(self.positional_embeddings.device)\n",
    "    tokens = self.linear_mapper(patches)\n",
    "\n",
    "    # Adding classification token to the tokens\n",
    "    tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n",
    "\n",
    "    # Adding positional embedding\n",
    "    pos_embed = self.pos_embed.repeat(n, 1, 1)\n",
    "    out = tokens + pos_embed\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b344cc93-1749-40fe-a4c1-bf04251aaf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_image,_ = test_set[0]\n",
    "tst_image = tst_image.unsqueeze(0)\n",
    "tst_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda28d5-56a8-4a0c-b52b-e047dfec7b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini = MyMiniViT()\n",
    "midi = MyMidiViT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded9635b-7fc1-47e8-beb5-fec11f27035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=midi(tst_image)\n",
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcdc881-4e14-4f2b-92c4-8d71dff65a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is multi-head self-attention submodule\n",
    "\n",
    "class MyMSA(nn.Module):\n",
    "    def __init__(self, d, n_heads=2):\n",
    "        super(MyMSA, self).__init__()\n",
    "        self.d = d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
    "\n",
    "        d_head = int(d / n_heads)\n",
    "        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.d_head = d_head\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        # Sequences has shape (N, seq_length, token_dim)\n",
    "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "        result = []\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            #import ipdb; ipdb.set_trace()\n",
    "            for head in range(self.n_heads):\n",
    "                q_mapping = self.q_mappings[head]\n",
    "                k_mapping = self.k_mappings[head]\n",
    "                v_mapping = self.v_mappings[head]\n",
    "\n",
    "                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "                seq_result.append(attention @ v)\n",
    "            result.append(torch.hstack(seq_result))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n",
    "\n",
    "\n",
    "# This is multi-head self-attention submodule packaged in  block with a residual connection\n",
    "\n",
    "class MyViTBlock(nn.Module):\n",
    "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
    "        super(MyViTBlock, self).__init__()\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_d)\n",
    "        self.mhsa = MyMSA(hidden_d, n_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.mhsa(self.norm1(x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f830d1af-6c8a-4e91-ac04-2260b44c9d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete ViT\n",
    "\n",
    "class MyViT(nn.Module):\n",
    "    def __init__(self, chw, n_patches=7, n_blocks=2, embed_dim = 32, hidden_d=8, n_heads=2, out_d=10):\n",
    "        # Super constructor\n",
    "        super(MyViT, self).__init__()\n",
    "        \n",
    "        # Attributes\n",
    "        self.chw = chw # ( C , H , W )\n",
    "        self.n_patches = n_patches\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_d = hidden_d\n",
    "        \n",
    "        # Input and patches sizes\n",
    "        assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        self.patch_size = (chw[1] // n_patches, chw[2] // n_patches)\n",
    "\n",
    "        # 1) Projector and Linear mapper (perhaps we can get by without the Linear mapper?)\n",
    "        in_chans = chw[0]\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim,\n",
    "                              kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        self.linear_mapper = nn.Linear(embed_dim, hidden_d)\n",
    "\n",
    "        \n",
    "        # 2) Learnable classification token\n",
    "        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "        \n",
    "        # 3) Positional embedding\n",
    "        self.register_buffer('positional_embeddings', get_positional_embeddings(n_patches ** 2 + 1, hidden_d), persistent=False)\n",
    "        \n",
    "        # 4) Transformer encoder blocks\n",
    "        self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n",
    "        \n",
    "        # 5) Classification MLPk\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.hidden_d, out_d),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Dividing images into patches\n",
    "        n, c, h, w = images.shape\n",
    "        patches = self.proj(images).flatten(2).transpose(1, 2).to(self.positional_embeddings.device)\n",
    "        tokens = self.linear_mapper(patches)\n",
    "\n",
    "        # Adding classification token to the tokens\n",
    "        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
    "        \n",
    "        # Adding positional embedding\n",
    "        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
    "        \n",
    "        # Transformer Blocks\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "            \n",
    "        # Getting the classification token only\n",
    "        out = out[:, 0]\n",
    "        \n",
    "        return self.mlp(out) # Map to output dimension, output category distribution\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8af8bd-bdc3-47ca-a76e-e0d33405c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxi = MyViT((1, 28, 28), n_patches=7, n_blocks=2, embed_dim = 32, hidden_d=8, n_heads=2, out_d=10)\n",
    "out=maxi(tst_image)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112061a6-f839-42e9-a05f-7049348ed7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21c1d5f-251c-4d02-8ec7-a27faf88f0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
