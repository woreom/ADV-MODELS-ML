{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n",
      "0.16.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataset import Subset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models.vgg import vgg16\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: data//CIFAR10/train/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomCrop(size=(32, 32), padding=4)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))\n",
      "           )\n",
      "    len:50000\n",
      "    shape:(32, 32, 3)\n",
      "    classes:{'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: data//CIFAR10/test/\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomCrop(size=(32, 32), padding=4)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))\n",
      "           )\n",
      "    len:10000\n",
      "    shape:(32, 32, 3)\n",
      "    classes:{'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "  transforms.RandomCrop(32, padding=4),\n",
    "  transforms.RandomHorizontalFlip(),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(\n",
    "      (0.4914, 0.4822, 0.4465),\n",
    "      (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "START_PATH = \"data/\"\n",
    "create_folder(f\"{START_PATH}/CIFAR10/\")\n",
    "\n",
    "train_data = CIFAR10(root=f\"{START_PATH}/CIFAR10/train/\",\n",
    "                    train=True, \n",
    "                    download=True,\n",
    "                    transform=transform)\n",
    "\n",
    "print(train_data)\n",
    "print(f\"    len:{len(train_data)}\")\n",
    "print(f\"    shape:{train_data.data.shape[1:]}\")\n",
    "print(f\"    classes:{train_data.class_to_idx}\")\n",
    "\n",
    "trainset, valset = random_split(\n",
    "                      train_data, \n",
    "                      [40000, 10000])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "testset = CIFAR10(root=f\"{START_PATH}/CIFAR10/test/\", \n",
    "                    train=False, \n",
    "                    download=True,\n",
    "                    transform=transform)\n",
    "print(testset)\n",
    "print(f\"    len:{len(testset)}\")\n",
    "print(f\"    shape:{testset.data.shape[1:]}\")\n",
    "print(f\"    classes:{testset.class_to_idx}\")\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(constrained_layout=True)\n",
    "fig.suptitle('Checking Images')\n",
    "\n",
    "A = CIFAR10(root=f\"{START_PATH}/CIFAR10/train/\",\n",
    "                    train=True, \n",
    "                    download=True,)\n",
    "B = CIFAR10(root=f\"{START_PATH}/CIFAR10/test/\", \n",
    "                    train=False, \n",
    "                    download=True,)\n",
    "\n",
    "subfigs = fig.subfigures(nrows=2, ncols=1)\n",
    "for row, subfig in enumerate(subfigs):\n",
    "    dataset = B if row else A\n",
    "    subfig.suptitle(f'{\"test\" if row else \"train\"}')\n",
    "    axs = subfig.subplots(nrows=1, ncols=2)\n",
    "    for col, ax in enumerate(axs):\n",
    "        ax.imshow(dataset[col][0])\n",
    "        ax.set_title(f'Sample {col}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "VGG                                      [1, 1000]                 --\n",
      "Sequential: 1-1                        [1, 512, 7, 7]            --\n",
      "    Conv2d: 2-1                       [1, 64, 224, 224]         1,792\n",
      "    ReLU: 2-2                         [1, 64, 224, 224]         --\n",
      "    Conv2d: 2-3                       [1, 64, 224, 224]         36,928\n",
      "    ReLU: 2-4                         [1, 64, 224, 224]         --\n",
      "    MaxPool2d: 2-5                    [1, 64, 112, 112]         --\n",
      "    Conv2d: 2-6                       [1, 128, 112, 112]        73,856\n",
      "    ReLU: 2-7                         [1, 128, 112, 112]        --\n",
      "    Conv2d: 2-8                       [1, 128, 112, 112]        147,584\n",
      "    ReLU: 2-9                         [1, 128, 112, 112]        --\n",
      "    MaxPool2d: 2-10                   [1, 128, 56, 56]          --\n",
      "    Conv2d: 2-11                      [1, 256, 56, 56]          295,168\n",
      "    ReLU: 2-12                        [1, 256, 56, 56]          --\n",
      "    Conv2d: 2-13                      [1, 256, 56, 56]          590,080\n",
      "    ReLU: 2-14                        [1, 256, 56, 56]          --\n",
      "    Conv2d: 2-15                      [1, 256, 56, 56]          590,080\n",
      "    ReLU: 2-16                        [1, 256, 56, 56]          --\n",
      "    MaxPool2d: 2-17                   [1, 256, 28, 28]          --\n",
      "    Conv2d: 2-18                      [1, 512, 28, 28]          1,180,160\n",
      "    ReLU: 2-19                        [1, 512, 28, 28]          --\n",
      "    Conv2d: 2-20                      [1, 512, 28, 28]          2,359,808\n",
      "    ReLU: 2-21                        [1, 512, 28, 28]          --\n",
      "    Conv2d: 2-22                      [1, 512, 28, 28]          2,359,808\n",
      "    ReLU: 2-23                        [1, 512, 28, 28]          --\n",
      "    MaxPool2d: 2-24                   [1, 512, 14, 14]          --\n",
      "    Conv2d: 2-25                      [1, 512, 14, 14]          2,359,808\n",
      "    ReLU: 2-26                        [1, 512, 14, 14]          --\n",
      "    Conv2d: 2-27                      [1, 512, 14, 14]          2,359,808\n",
      "    ReLU: 2-28                        [1, 512, 14, 14]          --\n",
      "    Conv2d: 2-29                      [1, 512, 14, 14]          2,359,808\n",
      "    ReLU: 2-30                        [1, 512, 14, 14]          --\n",
      "    MaxPool2d: 2-31                   [1, 512, 7, 7]            --\n",
      "AdaptiveAvgPool2d: 1-2                 [1, 512, 7, 7]            --\n",
      "Sequential: 1-3                        [1, 1000]                 --\n",
      "    Linear: 2-32                      [1, 4096]                 102,764,544\n",
      "    ReLU: 2-33                        [1, 4096]                 --\n",
      "    Dropout: 2-34                     [1, 4096]                 --\n",
      "    Linear: 2-35                      [1, 4096]                 16,781,312\n",
      "    ReLU: 2-36                        [1, 4096]                 --\n",
      "    Dropout: 2-37                     [1, 4096]                 --\n",
      "    Linear: 2-38                      [1, 1000]                 4,097,000\n",
      "==========================================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 15.48\n",
      "==========================================================================================\n",
      "Input size (MB): 0.60\n",
      "Forward/backward pass size (MB): 108.45\n",
      "Params size (MB): 553.43\n",
      "Estimated Total Size (MB): 662.49\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device=device)\n",
    "print(summary(model, input_size=(1, 3, 224, 224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VGG                                      [1, 10]                   --\n",
       "Sequential: 1-1                        [1, 512, 1, 1]            --\n",
       "    Conv2d: 2-1                       [1, 64, 32, 32]           1,792\n",
       "    ReLU: 2-2                         [1, 64, 32, 32]           --\n",
       "    Conv2d: 2-3                       [1, 64, 32, 32]           36,928\n",
       "    ReLU: 2-4                         [1, 64, 32, 32]           --\n",
       "    MaxPool2d: 2-5                    [1, 64, 16, 16]           --\n",
       "    Conv2d: 2-6                       [1, 128, 16, 16]          73,856\n",
       "    ReLU: 2-7                         [1, 128, 16, 16]          --\n",
       "    Conv2d: 2-8                       [1, 128, 16, 16]          147,584\n",
       "    ReLU: 2-9                         [1, 128, 16, 16]          --\n",
       "    MaxPool2d: 2-10                   [1, 128, 8, 8]            --\n",
       "    Conv2d: 2-11                      [1, 256, 8, 8]            295,168\n",
       "    ReLU: 2-12                        [1, 256, 8, 8]            --\n",
       "    Conv2d: 2-13                      [1, 256, 8, 8]            590,080\n",
       "    ReLU: 2-14                        [1, 256, 8, 8]            --\n",
       "    Conv2d: 2-15                      [1, 256, 8, 8]            590,080\n",
       "    ReLU: 2-16                        [1, 256, 8, 8]            --\n",
       "    MaxPool2d: 2-17                   [1, 256, 4, 4]            --\n",
       "    Conv2d: 2-18                      [1, 512, 4, 4]            1,180,160\n",
       "    ReLU: 2-19                        [1, 512, 4, 4]            --\n",
       "    Conv2d: 2-20                      [1, 512, 4, 4]            2,359,808\n",
       "    ReLU: 2-21                        [1, 512, 4, 4]            --\n",
       "    Conv2d: 2-22                      [1, 512, 4, 4]            2,359,808\n",
       "    ReLU: 2-23                        [1, 512, 4, 4]            --\n",
       "    MaxPool2d: 2-24                   [1, 512, 2, 2]            --\n",
       "    Conv2d: 2-25                      [1, 512, 2, 2]            2,359,808\n",
       "    ReLU: 2-26                        [1, 512, 2, 2]            --\n",
       "    Conv2d: 2-27                      [1, 512, 2, 2]            2,359,808\n",
       "    ReLU: 2-28                        [1, 512, 2, 2]            --\n",
       "    Conv2d: 2-29                      [1, 512, 2, 2]            2,359,808\n",
       "    ReLU: 2-30                        [1, 512, 2, 2]            --\n",
       "    MaxPool2d: 2-31                   [1, 512, 1, 1]            --\n",
       "AdaptiveAvgPool2d: 1-2                 [1, 512, 7, 7]            --\n",
       "Sequential: 1-3                        [1, 10]                   --\n",
       "    Linear: 2-32                      [1, 4096]                 102,764,544\n",
       "    ReLU: 2-33                        [1, 4096]                 --\n",
       "    Dropout: 2-34                     [1, 4096]                 --\n",
       "    Linear: 2-35                      [1, 4096]                 16,781,312\n",
       "    ReLU: 2-36                        [1, 4096]                 --\n",
       "    Dropout: 2-37                     [1, 4096]                 --\n",
       "    Linear: 2-38                      [1, 10]                   40,970\n",
       "==========================================================================================\n",
       "Total params: 134,301,514\n",
       "Trainable params: 134,301,514\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 433.06\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 2.28\n",
       "Params size (MB): 537.21\n",
       "Estimated Total Size (MB): 539.50\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier[-1] = nn.Linear(4096, 10)\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device=device)\n",
    "\n",
    "summary(model, input_size=(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Why does the vgg16.classifier have the same output dimensions in both cases, when you have different input sizes?\n",
    "\n",
    "    For CIFAR-10 vgg16 Sequential 1-1 output dimensions is [batch_size, 512, 1, 1] and For ImageNet vgg16 Sequential 1-1 output dimensions is [batch_size, 512, 7, 7].\n",
    "\n",
    "    Then there is an AdaptiveAvgPool2d(7) where it's functionally is to reshape the vector to [batch_size, X, 7, 7] which in case of vgg16 it's X=512.\n",
    "\n",
    "* Is the first sequential layer identical in these two summaries?\n",
    "\n",
    "    It's the same sequential layer in the sense of layer types and parameters but the output sizes are different.\n",
    "\n",
    "* Cell 2 will replace the very last layer of the vgg model (why?), then map it to the available devices memory under a new name  model.\n",
    "\n",
    "    Since the Sequential: 1-3 is the classifier the last layer is equal to the number of classes in our dataset CIFAR-10 has 10 hence we need to change the last layer to nn.Linear(last_layer_output, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.07it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 0.3020119071006775 Val Loss: 0.4083535075187683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.29it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: 0.30716444104909896 Val Loss: 0.4037517309188843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.33it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train Loss: 0.30148896425962446 Val Loss: 0.4112278789281845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.39it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Train Loss: 0.2963780641555786 Val Loss: 0.40599641799926756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.34it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Train Loss: 0.28627773858606814 Val Loss: 0.40190635323524476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.23it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Train Loss: 0.2941130816936493 Val Loss: 0.4004033476114273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.38it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Train Loss: 0.29180846735835075 Val Loss: 0.40014722645282746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.31it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Train Loss: 0.2895826391875744 Val Loss: 0.40393871665000913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.39it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Train Loss: 0.27536170110106467 Val Loss: 0.3990707516670227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.34it/s]\n",
      "100%|| 10/10 [00:01<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Train Loss: 0.2811198852956295 Val Loss: 0.3996320515871048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.36it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Train Loss: 0.27977788671851156 Val Loss: 0.38713781237602235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.42it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Train Loss: 0.26965288147330285 Val Loss: 0.39935589134693145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.33it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Train Loss: 0.2706278458237648 Val Loss: 0.40706627368927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.20it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Train Loss: 0.26820439621806147 Val Loss: 0.3976297855377197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.34it/s]\n",
      "100%|| 10/10 [00:01<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Train Loss: 0.2673827975988388 Val Loss: 0.3897610008716583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.33it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 Train Loss: 0.2632719587534666 Val Loss: 0.38220682740211487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.27it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 Train Loss: 0.2517806399613619 Val Loss: 0.39488222599029543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.22it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 Train Loss: 0.2557398406788707 Val Loss: 0.38958999514579773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.32it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 Train Loss: 0.24728500992059707 Val Loss: 0.38785726130008696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.27it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Train Loss: 0.2474047277122736 Val Loss: 0.3919557839632034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.31it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 Train Loss: 0.2514912519603968 Val Loss: 0.39855829775333407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.24it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 Train Loss: 0.24869242459535598 Val Loss: 0.38737634718418124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.35it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 Train Loss: 0.24061160311102867 Val Loss: 0.4046350955963135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.30it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 Train Loss: 0.2392257984727621 Val Loss: 0.3998374342918396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.28it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 Train Loss: 0.2370744414627552 Val Loss: 0.39287797808647157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.23it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 Train Loss: 0.23790876641869546 Val Loss: 0.3905238449573517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.27it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 Train Loss: 0.23359871804714202 Val Loss: 0.40204165279865267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.35it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 Train Loss: 0.22854421213269233 Val Loss: 0.39694699048995974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.39it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 Train Loss: 0.22584932073950767 Val Loss: 0.38599860668182373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.33it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 Train Loss: 0.22925261668860913 Val Loss: 0.39943075776100156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.30it/s]\n",
      "100%|| 10/10 [00:01<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 Train Loss: 0.22594149336218833 Val Loss: 0.3970495581626892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.36it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 Train Loss: 0.22431258000433446 Val Loss: 0.39053178429603574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.29it/s]\n",
      "100%|| 10/10 [00:01<00:00,  6.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 Train Loss: 0.2200208619236946 Val Loss: 0.39362879395484923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.40it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 Train Loss: 0.21393540278077125 Val Loss: 0.3963227719068527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.31it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 Train Loss: 0.21499963514506817 Val Loss: 0.39359669387340546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.32it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 Train Loss: 0.21080552227795124 Val Loss: 0.392208456993103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.28it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 Train Loss: 0.20981862619519234 Val Loss: 0.3865497589111328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.28it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 Train Loss: 0.20855379588901996 Val Loss: 0.38810268938541415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.24it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 Train Loss: 0.20995039716362954 Val Loss: 0.39432379603385925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:07<00:00,  5.25it/s]\n",
      "100%|| 10/10 [00:01<00:00,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 Train Loss: 0.20906747579574586 Val Loss: 0.3906974226236343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 40\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=0.001, \n",
    "                      momentum=0.9)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    # Training \n",
    "    train_loss = 0.0\n",
    "    model.train() # <1>\n",
    "    for inputs, labels in tqdm(trainloader):\n",
    "        labels = labels.reshape(-1,1)\n",
    "        y_onehot = torch.FloatTensor(labels.shape[0], 10)\n",
    "\n",
    "        # In your for loop\n",
    "        y_onehot.zero_()\n",
    "        y_onehot.scatter_(1, labels, 1)\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        y_onehot = y_onehot.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, y_onehot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0.0\n",
    "    model.eval() # <2>\n",
    "    for inputs, labels in tqdm(valloader):\n",
    "        labels = labels.reshape(-1,1)\n",
    "        y_onehot = torch.FloatTensor(labels.shape[0], 10)\n",
    "\n",
    "        # In your for loop\n",
    "        y_onehot.zero_()\n",
    "        y_onehot.scatter_(1, labels, 1)\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        y_onehot = y_onehot.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, y_onehot)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    print(\"Epoch: {} Train Loss: {} Val Loss: {}\".format(\n",
    "                  epoch, \n",
    "                  train_loss/len(trainloader), \n",
    "                  val_loss/len(valloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1024\n",
      "Test Accuracy: 0.850878894329071\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0.0\n",
    "for x_test_batch, y_test_batch in testloader:\n",
    "    model.eval() \n",
    "    y_test_batch = y_test_batch.to(device)\n",
    "    x_test_batch = x_test_batch.to(device) \n",
    "    y_pred_batch = model(x_test_batch)\n",
    "    _, predicted = torch.max(y_pred_batch, 1) \n",
    "    num_correct += (predicted == y_test_batch).float().sum()\n",
    "\n",
    "accuracy = num_correct/(len(testloader)*testloader.batch_size)\n",
    "print(len(testloader), testloader.batch_size) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) # <1>\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "LeNet5                                   [1, 10]                   --\n",
      "Conv2d: 1-1                            [1, 6, 28, 28]            456\n",
      "Conv2d: 1-2                            [1, 16, 10, 10]           2,416\n",
      "Linear: 1-3                            [1, 120]                  48,120\n",
      "Linear: 1-4                            [1, 84]                   10,164\n",
      "Linear: 1-5                            [1, 10]                   850\n",
      "==========================================================================================\n",
      "Total params: 62,006\n",
      "Trainable params: 62,006\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.66\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 0.25\n",
      "Estimated Total Size (MB): 0.31\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2528441/3090054245.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.fc3(x))\n"
     ]
    }
   ],
   "source": [
    "model = LeNet5()\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device=device)\n",
    "print(summary(model, input_size=(1, 3, 32, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/tmp/ipykernel_2528441/3090054245.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.fc3(x))\n",
      "100%|| 40/40 [00:05<00:00,  6.94it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 2.305186414718628 Val Loss: 2.3035022258758544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.93it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: 2.3022678434848785 Val Loss: 2.30046603679657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.82it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train Loss: 2.299144846200943 Val Loss: 2.297042465209961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  7.06it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Train Loss: 2.2955490052700043 Val Loss: 2.2928881645202637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.98it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Train Loss: 2.29013386964798 Val Loss: 2.287351655960083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.95it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Train Loss: 2.283600479364395 Val Loss: 2.279353213310242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  7.00it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Train Loss: 2.2734214067459106 Val Loss: 2.2673906564712523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.90it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Train Loss: 2.2580332159996033 Val Loss: 2.2513036251068117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.78it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Train Loss: 2.2387358963489534 Val Loss: 2.231567931175232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.86it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Train Loss: 2.2164505779743195 Val Loss: 2.2090608358383177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.86it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Train Loss: 2.1916605830192566 Val Loss: 2.180667519569397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.83it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Train Loss: 2.1653063178062437 Val Loss: 2.1560717582702638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  7.01it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Train Loss: 2.1390668034553526 Val Loss: 2.127865266799927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.84it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Train Loss: 2.110614961385727 Val Loss: 2.1046077966690064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.87it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Train Loss: 2.083615982532501 Val Loss: 2.0794065952301026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.88it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 Train Loss: 2.063385045528412 Val Loss: 2.0579543828964235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.84it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 Train Loss: 2.0461622178554535 Val Loss: 2.0411171197891234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.83it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 Train Loss: 2.0309163331985474 Val Loss: 2.0273284912109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.84it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 Train Loss: 2.0160944193601607 Val Loss: 2.015182042121887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.91it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Train Loss: 2.004433274269104 Val Loss: 2.004601168632507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.83it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 Train Loss: 1.9936614245176316 Val Loss: 1.9926472067832948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.88it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 Train Loss: 1.9865090072154998 Val Loss: 1.9836878418922423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.99it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 Train Loss: 1.973956334590912 Val Loss: 1.972481656074524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:06<00:00,  6.61it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 Train Loss: 1.9615156203508377 Val Loss: 1.962409996986389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.71it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 Train Loss: 1.9536800026893615 Val Loss: 1.9516275882720948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:06<00:00,  6.63it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 Train Loss: 1.9438543975353242 Val Loss: 1.9384382367134094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.69it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 Train Loss: 1.9340874016284944 Val Loss: 1.932640051841736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.89it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 Train Loss: 1.922800424695015 Val Loss: 1.9226879954338074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.93it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 Train Loss: 1.9128503918647766 Val Loss: 1.9076096534729003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.85it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 Train Loss: 1.9026036888360978 Val Loss: 1.902693247795105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.72it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 Train Loss: 1.8901843935251237 Val Loss: 1.8871679306030273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:06<00:00,  6.54it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 Train Loss: 1.8815083861351014 Val Loss: 1.875225555896759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.85it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 Train Loss: 1.867640596628189 Val Loss: 1.864507222175598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.85it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 Train Loss: 1.8565067678689957 Val Loss: 1.8563365697860719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.87it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 Train Loss: 1.8457136809825898 Val Loss: 1.8429678201675415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:06<00:00,  6.65it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 Train Loss: 1.8279571294784547 Val Loss: 1.8328643918037415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.79it/s]\n",
      "100%|| 10/10 [00:01<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 Train Loss: 1.8218814134597778 Val Loss: 1.8201382994651794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.88it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 Train Loss: 1.8164980709552765 Val Loss: 1.8097909092903137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.96it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 Train Loss: 1.803024199604988 Val Loss: 1.8037670373916626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [00:05<00:00,  6.95it/s]\n",
      "100%|| 10/10 [00:01<00:00,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 Train Loss: 1.7885792762041093 Val Loss: 1.786471700668335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 40\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=0.001, \n",
    "                      momentum=0.9)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    # Training \n",
    "    train_loss = 0.0\n",
    "    model.train() # <1>\n",
    "    for inputs, labels in tqdm(trainloader):\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(torch.log(outputs), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0.0\n",
    "    model.eval() # <2>\n",
    "    for inputs, labels in tqdm(valloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(torch.log(outputs), labels)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    print(\"Epoch: {} Train Loss: {} Val Loss: {}\".format(\n",
    "                  epoch, \n",
    "                  train_loss/len(trainloader), \n",
    "                  val_loss/len(valloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2528441/3090054245.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.fc3(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1024\n",
      "Test Accuracy: 0.3509765565395355\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0.0\n",
    "for x_test_batch, y_test_batch in testloader:\n",
    "    model.eval() \n",
    "    y_test_batch = y_test_batch.to(device)\n",
    "    x_test_batch = x_test_batch.to(device) \n",
    "    y_pred_batch = model(x_test_batch)\n",
    "    _, predicted = torch.max(y_pred_batch, 1) \n",
    "    num_correct += (predicted == y_test_batch).float().sum()\n",
    "\n",
    "accuracy = num_correct/(len(testloader)*testloader.batch_size)\n",
    "print(len(testloader), testloader.batch_size) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Question: Please compare the accuracy of Vgg16 on CIFAR10 with the accuracy we obtained with LeNet5. Why is one better than another?\n",
    "\n",
    "    The first difference between the models are that one is pretrained (vgg16) on miniImageNet and the other is not.\n",
    "    \n",
    "    The second difference between the models the number of parameters vgg16 is significantly larger than LeNet also vgg16 is more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: data//CIFAR10/train/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomCrop(size=(32, 32), padding=4)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))\n",
      "           )\n",
      "    len:50000\n",
      "    shape:(32, 32, 3)\n",
      "    classes:{'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: data//CIFAR10/test/\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomCrop(size=(32, 32), padding=4)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))\n",
      "           )\n",
      "    len:10000\n",
      "    shape:(32, 32, 3)\n",
      "    classes:{'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "  transforms.RandomCrop(32, padding=4),\n",
    "  transforms.RandomHorizontalFlip(),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(\n",
    "      (0.4914, 0.4822, 0.4465),\n",
    "      (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "START_PATH = \"data/\"\n",
    "create_folder(f\"{START_PATH}/CIFAR10/\")\n",
    "\n",
    "train_data = CIFAR10(root=f\"{START_PATH}/CIFAR10/train/\",\n",
    "                    train=True, \n",
    "                    download=True,\n",
    "                    transform=transform,)\n",
    "\n",
    "print(train_data)\n",
    "print(f\"    len:{len(train_data)}\")\n",
    "print(f\"    shape:{train_data.data.shape[1:]}\")\n",
    "print(f\"    classes:{train_data.class_to_idx}\")\n",
    "\n",
    "targets = [1, 3, 5, 9]\n",
    "indices = [i for i, label in enumerate(train_data.targets) if label in targets]\n",
    "\n",
    "train_data = Subset(train_data, indices)\n",
    "\n",
    "\n",
    "trainset, valset = random_split(\n",
    "                      train_data, \n",
    "                      [19500, 500])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "testset = CIFAR10(root=f\"{START_PATH}/CIFAR10/test/\", \n",
    "                    train=False, \n",
    "                    download=True,\n",
    "                    transform=transform)\n",
    "print(testset)\n",
    "print(f\"    len:{len(testset)}\")\n",
    "print(f\"    shape:{testset.data.shape[1:]}\")\n",
    "print(f\"    classes:{testset.class_to_idx}\")\n",
    "\n",
    "indices = [i for i, label in enumerate(testset.targets) if label in targets]\n",
    "testset = Subset(testset, indices)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "labmap = {x:i for i, x in enumerate(targets)}\n",
    "\n",
    "reindex_T = torchvision.transforms.Compose([\n",
    "                                 lambda x:torch.LongTensor([labmap[i.item()] for i in x])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)\n",
    "model.classifier[-1] = nn.Linear(4096, 4)\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:03<00:00,  5.24it/s]\n",
      "100%|| 1/1 [00:00<00:00, 17.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 1.0842735469341278 Val Loss: 0.6538663506507874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:03<00:00,  5.61it/s]\n",
      "100%|| 1/1 [00:00<00:00, 16.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Loss: 0.6301272094249726 Val Loss: 0.49326857924461365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:03<00:00,  5.61it/s]\n",
      "100%|| 1/1 [00:00<00:00, 13.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train Loss: 0.5042219743132591 Val Loss: 0.46651461720466614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:03<00:00,  5.71it/s]\n",
      "100%|| 1/1 [00:00<00:00, 17.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Train Loss: 0.4655347615480423 Val Loss: 0.394621342420578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:03<00:00,  5.79it/s]\n",
      "100%|| 1/1 [00:00<00:00, 16.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Train Loss: 0.42116420716047287 Val Loss: 0.3955713212490082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:03<00:00,  5.65it/s]\n",
      "100%|| 1/1 [00:00<00:00, 14.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Train Loss: 0.3978876531124115 Val Loss: 0.37294134497642517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:03<00:00,  5.55it/s]\n",
      "100%|| 1/1 [00:00<00:00, 17.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Train Loss: 0.3785146251320839 Val Loss: 0.3683062791824341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:03<00:00,  5.70it/s]\n",
      "100%|| 1/1 [00:00<00:00, 15.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Train Loss: 0.36358299404382705 Val Loss: 0.34197157621383667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:03<00:00,  5.38it/s]\n",
      "100%|| 1/1 [00:00<00:00, 14.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Train Loss: 0.36631740629673004 Val Loss: 0.35995039343833923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:03<00:00,  5.60it/s]\n",
      "100%|| 1/1 [00:00<00:00, 16.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Train Loss: 0.346533689647913 Val Loss: 0.35241562128067017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:03<00:00,  5.63it/s]\n",
      "100%|| 1/1 [00:00<00:00, 17.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Train Loss: 0.339718297123909 Val Loss: 0.30272620916366577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:03<00:00,  5.62it/s]\n",
      "100%|| 1/1 [00:00<00:00, 16.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Train Loss: 0.33701505661010744 Val Loss: 0.33656013011932373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:03<00:00,  5.39it/s]\n",
      "100%|| 1/1 [00:00<00:00, 15.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Train Loss: 0.3188662134110928 Val Loss: 0.3390875458717346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:03<00:00,  5.33it/s]\n",
      "100%|| 1/1 [00:00<00:00, 14.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Train Loss: 0.3202059805393219 Val Loss: 0.3200990557670593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:03<00:00,  5.48it/s]\n",
      "100%|| 1/1 [00:00<00:00, 15.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Train Loss: 0.3079750992357731 Val Loss: 0.2934420704841614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:03<00:00,  5.29it/s]\n",
      "100%|| 1/1 [00:00<00:00, 15.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 Train Loss: 0.30969926714897156 Val Loss: 0.29812148213386536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:03<00:00,  5.44it/s]\n",
      "100%|| 1/1 [00:00<00:00, 14.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 Train Loss: 0.31443904936313627 Val Loss: 0.2977924942970276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:03<00:00,  5.33it/s]\n",
      "100%|| 1/1 [00:00<00:00, 15.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 Train Loss: 0.29608548805117607 Val Loss: 0.2844195067882538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:03<00:00,  5.34it/s]\n",
      "100%|| 1/1 [00:00<00:00, 15.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 Train Loss: 0.28667439967393876 Val Loss: 0.28236061334609985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:03<00:00,  5.31it/s]\n",
      "100%|| 1/1 [00:00<00:00, 13.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Train Loss: 0.2926120921969414 Val Loss: 0.276605486869812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      lr=0.001, \n",
    "                      momentum=0.9)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    # Training \n",
    "    train_loss = 0.0\n",
    "    model.train() # <1>\n",
    "    for inputs, labels in tqdm(trainloader):\n",
    "        labels = reindex_T(labels)\n",
    "        labels = labels.reshape(-1,1)\n",
    "        y_onehot = torch.FloatTensor(labels.shape[0], 4)\n",
    "\n",
    "        # In your for loop\n",
    "        y_onehot.zero_()\n",
    "        y_onehot.scatter_(1, labels, 1)\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        y_onehot = y_onehot.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, y_onehot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0.0\n",
    "    model.eval() # <2>\n",
    "    for inputs, labels in tqdm(valloader):\n",
    "        labels = reindex_T(labels)\n",
    "        labels = labels.reshape(-1,1)\n",
    "        y_onehot = torch.FloatTensor(labels.shape[0], 4)\n",
    "\n",
    "        # In your for loop\n",
    "        y_onehot.zero_()\n",
    "        y_onehot.scatter_(1, labels, 1)\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        y_onehot = y_onehot.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, y_onehot)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    print(\"Epoch: {} Train Loss: {} Val Loss: {}\".format(\n",
    "                  epoch, \n",
    "                  train_loss/len(trainloader), \n",
    "                  val_loss/len(valloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1024\n",
      "Test Accuracy: 0.85205078125\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0.0\n",
    "for x_test_batch, y_test_batch in testloader:\n",
    "    model.eval() \n",
    "    y_test_batch = reindex_T(y_test_batch).to(device)\n",
    "    x_test_batch = x_test_batch.to(device) \n",
    "    y_pred_batch = model(x_test_batch)\n",
    "    _, predicted = torch.max(y_pred_batch, 1)\n",
    "    num_correct += (predicted == y_test_batch).float().sum()\n",
    "\n",
    "accuracy = num_correct/(len(testloader)*testloader.batch_size)\n",
    "print(len(testloader), testloader.batch_size) \n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, I have to say that \"discuss\" is vague verb it does not give any information to the reader of what is it you ask. A problem needs a solution, problem 3 doesn't have a problem nor a question nor a solution in it, which makes it impossible to understand. The only thing I understood was I needed to make MNIST dataset for it.\n",
    "\n",
    "Class Definition:\n",
    "\n",
    "The code defines a class named Cifar10_Cont_Dataset that inherits from torch.utils.data.Dataset. This class is designed to work with the Cifar10 dataset and is used in PyTorch for creating data loaders.\n",
    "\n",
    "Initialization (\\_\\_init\\_\\_ method):\n",
    "\n",
    "This method is called when an instance of the class is created.\n",
    "\n",
    "It takes three arguments:\n",
    "\n",
    "data_df: A pandas DataFrame containing the Cifar10 dataset (expected to have columns named 'label' and image data).\n",
    "\n",
    "transform: An optional function to apply transformations to the images (e.g., normalization, resizing).\n",
    "\n",
    "is_test: A boolean flag indicating whether it's test or training mode.\n",
    "\n",
    "The method initializes several attributes:\n",
    "\n",
    "    dataset: a list of [image sample, randomly selected negetive or positive sample, distance of the two samples (loss), label].\n",
    "\n",
    "    labels_positive: A dictionary that maps labels to lists of images with the same label (used only in training mode).\n",
    "\n",
    "    labels_negative: A dictionary that maps labels to lists of images with different labels (used only in training mode).\n",
    "    \n",
    "    If is_test is False (training mode), the method preprocesses the data by creating the labels_positive and labels_negative dictionaries. This involves iterating through the unique labels in the DataFrame and storing corresponding images in the respective dictionaries.\n",
    "\n",
    "ToPILImage: Convert a tensor or an ndarray to PIL Image\n",
    "\n",
    "Normalize: Normalize a tensor image with mean and standard deviation.\n",
    "\n",
    "ToTensor: Convert a PIL Image or ndarray to tensor and scale the values accordingly.\n",
    "\n",
    "ToTensor is necessary the other ones don't really matter that much. ToPILImage is usually used to see what has happened to the samples and normalize, normalize the data since neural networks work better with smaller numbers and backpropagation is more efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(\n",
    "      (0.4914, 0.4822, 0.4465),\n",
    "      (0.2023, 0.1994, 0.2010))])\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"data/MNIST/\",\n",
    "                    train=True, \n",
    "                    download=True,\n",
    "                    transform = train_transforms)\n",
    "length=len(train_dataset)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n",
      "6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=784, step=1)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, _ = random_split(train_dataset, [int(length/10), length - int(length/10)], torch.Generator().manual_seed(42))\n",
    "print(train_data.dataset.data[0].shape)\n",
    "print(len(train_data))\n",
    "df_t = pd.DataFrame(train_data.dataset.data[train_data.indices].reshape(len(train_data),np.prod(train_data.dataset.data[0].shape)))\n",
    "df_t.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026952</td>\n",
       "      <td>-0.016352</td>\n",
       "      <td>-0.012911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026952</td>\n",
       "      <td>-0.016352</td>\n",
       "      <td>-0.012911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026952</td>\n",
       "      <td>-0.016352</td>\n",
       "      <td>-0.012911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026952</td>\n",
       "      <td>-0.016352</td>\n",
       "      <td>-0.012911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026952</td>\n",
       "      <td>-0.016352</td>\n",
       "      <td>-0.012911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026952</td>\n",
       "      <td>-0.016352</td>\n",
       "      <td>-0.012911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026952</td>\n",
       "      <td>-0.016352</td>\n",
       "      <td>-0.012911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026952</td>\n",
       "      <td>-0.016352</td>\n",
       "      <td>-0.012911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026952</td>\n",
       "      <td>-0.016352</td>\n",
       "      <td>-0.012911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026952</td>\n",
       "      <td>-0.016352</td>\n",
       "      <td>-0.012911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows  785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label    0    1    2    3    4    5    6    7    8  ...       774  \\\n",
       "0         6  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.026952   \n",
       "1         7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.026952   \n",
       "2         6  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.026952   \n",
       "3         9  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.026952   \n",
       "4         4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.026952   \n",
       "...     ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...       ...   \n",
       "5995      3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.026952   \n",
       "5996      4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.026952   \n",
       "5997      5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.026952   \n",
       "5998      0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.026952   \n",
       "5999      7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.026952   \n",
       "\n",
       "           775       776  777  778  779  780  781  782  783  \n",
       "0    -0.016352 -0.012911  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1    -0.016352 -0.012911  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2    -0.016352 -0.012911  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3    -0.016352 -0.012911  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4    -0.016352 -0.012911  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...        ...       ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "5995 -0.016352 -0.012911  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "5996 -0.016352 -0.012911  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "5997 -0.016352 -0.012911  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "5998 -0.016352 -0.012911  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "5999 -0.016352 -0.012911  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[6000 rows x 785 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create a scaler object\n",
    "std_scaler = StandardScaler()\n",
    "std_scaler\n",
    "# fit and transform the data\n",
    "df_std = pd.DataFrame(std_scaler.fit_transform(df_t), columns=df_t.columns)\n",
    "df_std.insert(0, 'label',[train_data.dataset.targets[i].item() for i in train_data.indices])\n",
    "df_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:47: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:47: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "/tmp/ipykernel_2528441/2297181560.py:47: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if second is not -1:\n"
     ]
    }
   ],
   "source": [
    "class MNIST_Cont_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_df: pd.DataFrame, transform=None, is_test=False):\n",
    "        # method will run once when class object is created.\n",
    "        super(MNIST_Cont_Dataset, self).__init__()\n",
    "        dataset = []\n",
    "        labels_positive = {}\n",
    "        labels_negative = {}\n",
    "        if is_test == False:\n",
    "            # for each label create a set of same label images.\n",
    "            for i in list(data_df.label.unique()):\n",
    "                labels_positive[i] = data_df[data_df.label == i].to_numpy()\n",
    "            # for each label create a set of image of different label.\n",
    "            for i in list(data_df.label.unique()):\n",
    "                labels_negative[i] = data_df[data_df.label != i].to_numpy()\n",
    "\n",
    "        for i, row in tqdm(data_df.iterrows(), total=len(data_df)):\n",
    "            data = row.to_numpy()\n",
    "            # if test then only image will be returned.\n",
    "            if is_test:\n",
    "                label = -1\n",
    "                first = np.asarray(data[1:]).reshape(28, 28)\n",
    "                second = -1\n",
    "                dis = -1\n",
    "            else:\n",
    "                # label and image of the index for each row in df\n",
    "                label = data[0]\n",
    "                first = np.asarray(data[1:]).reshape(28, 28)\n",
    "                # probability of same label image == 0.5\n",
    "                if np.random.randint(0, 2) == 0:\n",
    "                    # randomly select same label image\n",
    "                    second = labels_positive[label][\n",
    "                        np.random.randint(0, len(labels_positive[label]))\n",
    "                    ]\n",
    "                else:\n",
    "                    # randomly select different(negative) label \n",
    "                    second = labels_negative[label][\n",
    "                        np.random.randint(0, len(labels_negative[label]))\n",
    "                    ]\n",
    "                # cosine is 1 for same and 0 for different label\n",
    "                dis = 1.0 if second[0] == label else 0.0\n",
    "                # reshape image\n",
    "                second = np.asarray(second[1:]).reshape(28, 28)\n",
    "\n",
    "            # apply transform on both images\n",
    "            if transform != None:\n",
    "                first = transform(first.astype(np.float32))\n",
    "                if second is not -1:\n",
    "                    second = transform(second.astype(np.float32))\n",
    "\n",
    "            # append to dataset list. \n",
    "            # this random list is created once and used in every epoch\n",
    "            dataset.append((first, second, dis, label))\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6000/6000 [00:00<00:00, 20045.98it/s]\n"
     ]
    }
   ],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "  # MNIST samples don't have 3 channels so this normalizes doesnt work for them\n",
    "  # transforms.Normalize(\n",
    "  #     (0.4914, 0.4822, 0.4465),\n",
    "  #     (0.2023, 0.1994, 0.2010))\n",
    "  ])\n",
    "cont_dataset = MNIST_Cont_Dataset(df_std, transform=train_transforms, is_test = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.9176e-02,\n",
       "          -2.5940e-02, -2.5779e-02, -3.1474e-02, -2.6694e-02, -3.3916e-02,\n",
       "          -2.8713e-02, -2.6726e-02, -2.8346e-02, -2.3016e-02, -1.6706e-02,\n",
       "          -1.2911e-02, -1.2911e-02, -1.2911e-02, -1.2911e-02,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.2911e-02,\n",
       "          -1.2911e-02,  0.0000e+00, -1.2911e-02, -1.3903e-02, -4.4102e-02,\n",
       "          -6.3960e-02, -7.8035e-02, -9.8120e-02, -1.1828e-01, -1.3359e-01,\n",
       "          -1.4207e-01, -1.3674e-01, -1.2964e-01, -1.1159e-01, -9.0272e-02,\n",
       "          -7.1527e-02, -5.8973e-02, -4.6033e-02, -2.2656e-02, -1.2911e-02,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.2911e-02, -1.7241e-02,\n",
       "          -2.4673e-02, -3.1852e-02, -4.6284e-02, -6.8639e-02, -1.0393e-01,\n",
       "          -1.3221e-01, -1.6399e-01, -1.9143e-01, -2.1645e-01, -2.4199e-01,\n",
       "          -2.5219e-01, -2.5393e-01,  3.6940e-01,  1.4283e+00,  6.6817e+00,\n",
       "           9.0774e+00,  1.1317e+01,  1.3687e+01, -4.2039e-02, -2.1931e-02,\n",
       "          -1.2911e-02,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.5507e-02, -2.0172e-02,\n",
       "          -5.3984e-02, -8.0049e-02, -1.2224e-01, -1.6791e-01, -2.1339e-01,\n",
       "          -2.6985e-01, -3.2996e-01, -3.9506e-01, -4.5540e-01, -5.0399e-01,\n",
       "          -5.3213e-01,  8.5071e-01,  1.7417e+00,  3.0501e+00,  3.6314e+00,\n",
       "           4.4952e+00,  6.0826e+00,  6.9691e+00, -9.5593e-02, -6.6610e-02,\n",
       "          -3.9739e-02, -1.7659e-02, -1.2911e-02],\n",
       "         [ 0.0000e+00,  0.0000e+00, -1.8106e-02, -2.9180e-02, -5.8009e-02,\n",
       "          -9.7757e-02, -1.4923e-01, -2.0462e-01, -2.6563e-01, -3.4022e-01,\n",
       "          -4.2883e-01, -5.2468e-01, -5.6512e-01, -6.3979e-01, -8.4430e-02,\n",
       "           9.3693e-01,  1.4241e+00,  1.6220e+00,  1.8948e+00,  2.2802e+00,\n",
       "           2.8419e+00,  3.7738e+00,  5.0103e+00,  2.8808e+00, -1.2742e-01,\n",
       "          -7.5766e-02, -3.1775e-02, -1.2911e-02],\n",
       "         [ 0.0000e+00, -1.2911e-02, -1.5586e-02, -3.9114e-02, -8.1819e-02,\n",
       "          -1.4535e-01, -2.1041e-01, -2.8187e-01, -3.6459e-01, -4.6201e-01,\n",
       "          -5.7282e-01, -4.4653e-01,  8.2407e-01,  1.2786e+00,  1.1486e+00,\n",
       "           1.1011e+00,  1.1370e+00,  1.2383e+00,  1.3997e+00,  1.6845e+00,\n",
       "           2.1043e+00,  2.8057e+00,  2.9427e+00, -2.3725e-01, -1.6588e-01,\n",
       "          -1.0277e-01, -5.2271e-02, -1.2911e-02],\n",
       "         [ 0.0000e+00, -1.6307e-02, -3.1119e-02, -6.2291e-02, -1.1818e-01,\n",
       "          -1.8503e-01, -2.6077e-01, -3.5161e-01, -4.5071e-01, -5.8068e-01,\n",
       "           4.2905e-01,  7.8460e-01,  1.2698e+00,  1.1406e+00,  1.0813e+00,\n",
       "           1.0605e+00,  1.0948e+00,  1.1561e+00,  1.2550e+00,  1.4493e+00,\n",
       "           1.7853e+00,  2.3539e+00,  9.1781e-01, -2.8111e-01, -1.9153e-01,\n",
       "          -1.1294e-01, -5.5939e-02, -1.8225e-02],\n",
       "         [-1.2911e-02, -2.1510e-02, -5.0715e-02, -8.4582e-02, -1.4387e-01,\n",
       "          -2.1651e-01, -3.0432e-01, -3.9706e-01, -3.1383e-01,  5.6833e-01,\n",
       "           1.4421e+00,  1.3010e+00,  1.1827e+00,  1.1645e+00,  1.1684e+00,\n",
       "           1.1855e+00,  1.2046e+00,  1.1975e+00,  1.1170e+00, -3.7067e-01,\n",
       "          -3.0130e-01, -7.4369e-02, -3.9401e-01, -2.9383e-01, -1.9305e-01,\n",
       "          -1.0922e-01, -5.0680e-02, -1.7607e-02],\n",
       "         [ 0.0000e+00, -1.8840e-02, -5.7865e-02, -9.2093e-02, -1.5650e-01,\n",
       "          -2.2960e-01, -3.2744e-01, -3.1009e-01,  2.1635e+00,  1.7085e+00,\n",
       "           1.4150e+00,  1.2694e+00,  1.2611e+00,  1.3558e+00,  1.4175e+00,\n",
       "           1.4425e+00,  7.0051e-01,  5.4603e-01, -4.7612e-01, -8.8265e-01,\n",
       "          -7.2724e-01, -5.6329e-01, -4.2046e-01, -2.9180e-01, -1.8350e-01,\n",
       "          -9.9339e-02, -4.4173e-02,  0.0000e+00],\n",
       "         [ 0.0000e+00, -2.2130e-02, -5.1649e-02, -9.2287e-02, -1.4960e-01,\n",
       "          -2.2982e-01, -3.2921e-01, -3.2453e-01,  2.0645e+00,  1.6412e+00,\n",
       "           1.3944e+00,  1.3435e+00,  1.4334e+00,  1.6021e+00,  1.1212e+00,\n",
       "           1.9948e-01, -7.4886e-01, -9.2543e-01, -9.3095e-01, -8.3793e-01,\n",
       "          -6.9022e-01, -5.3961e-01, -3.9855e-01, -2.7472e-01, -1.6428e-01,\n",
       "          -9.1204e-02, -3.6137e-02,  0.0000e+00],\n",
       "         [ 0.0000e+00, -2.4455e-02, -4.6926e-02, -8.7883e-02, -1.4133e-01,\n",
       "          -4.1857e-02,  1.7048e+00,  2.1952e+00,  1.9473e+00,  1.5561e+00,\n",
       "           1.3809e+00,  1.4134e+00,  1.4362e+00,  1.2476e+00, -5.2373e-01,\n",
       "          -7.9794e-01, -8.9809e-01, -9.5863e-01, -9.2944e-01, -5.5349e-01,\n",
       "          -4.8205e-01, -5.0142e-01, -3.7521e-01, -2.6297e-01, -1.5585e-01,\n",
       "          -6.5685e-02, -2.7645e-02, -1.2911e-02],\n",
       "         [ 0.0000e+00, -2.0389e-02, -3.2903e-02, -7.5266e-02, -1.2842e-01,\n",
       "           1.3326e+00,  3.5393e+00,  2.4599e+00,  1.8358e+00,  1.4991e+00,\n",
       "           1.3978e+00,  1.1782e+00, -1.6401e-01, -6.6933e-01,  4.2712e-02,\n",
       "          -6.0146e-02, -1.6065e-01, -1.7987e-01, -8.1510e-02,  1.5189e+00,\n",
       "           1.3072e+00,  6.7906e-01,  7.2184e-01, -2.5527e-01, -1.5458e-01,\n",
       "          -5.7072e-02, -1.8564e-02, -1.2911e-02],\n",
       "         [ 0.0000e+00, -1.5894e-02, -3.2841e-02, -5.9466e-02, -1.2713e-01,\n",
       "           1.2638e+00,  3.3697e+00,  2.3176e+00,  1.7596e+00,  1.4967e+00,\n",
       "           1.3146e+00, -2.7604e-01,  8.3390e-02,  6.7397e-01,  1.2385e+00,\n",
       "           1.1641e+00,  1.1285e+00,  1.1447e+00,  1.3119e+00,  1.6701e+00,\n",
       "           2.1176e+00,  2.5940e+00,  2.9966e+00,  1.3614e+00, -1.5745e-01,\n",
       "          -6.4377e-02, -2.4703e-02,  0.0000e+00],\n",
       "         [ 0.0000e+00, -1.5405e-02, -2.6326e-02, -4.7872e-02, -1.3496e-01,\n",
       "           4.5913e+00,  3.1237e+00,  2.2125e+00,  1.7307e+00,  1.5031e+00,\n",
       "           1.4327e+00,  1.2184e+00,  1.3259e+00,  1.2295e+00,  1.0801e+00,\n",
       "           1.0316e+00,  1.0589e+00,  1.1218e+00,  1.3264e+00,  1.6685e+00,\n",
       "           2.0624e+00,  2.4581e+00,  3.1455e+00,  4.1743e+00,  1.5777e+00,\n",
       "          -7.5762e-02, -1.9122e-02,  0.0000e+00],\n",
       "         [-1.2911e-02, -1.2911e-02, -2.2975e-02, -5.3478e-02, -1.4668e-01,\n",
       "           4.4801e+00,  2.8958e+00,  2.1646e+00,  1.7673e+00,  1.5744e+00,\n",
       "           1.5183e+00,  1.4747e+00,  1.3512e+00,  1.1546e+00,  1.0425e+00,\n",
       "           1.0336e+00,  1.0845e+00,  1.1640e+00,  1.3860e+00,  1.6715e+00,\n",
       "           2.0163e+00,  2.4081e+00,  3.1000e+00,  4.4491e+00,  1.7715e+00,\n",
       "          -8.3040e-02, -2.8194e-02,  0.0000e+00],\n",
       "         [-1.2911e-02, -1.2911e-02, -2.1765e-02, -6.0894e-02, -1.6452e-01,\n",
       "           4.1336e+00,  2.7922e+00,  2.1822e+00,  1.8401e+00,  1.6751e+00,\n",
       "           1.6117e+00,  1.5415e+00,  1.3836e+00,  1.2157e+00,  1.1233e+00,\n",
       "           1.1268e+00,  1.1829e+00,  1.2769e+00,  1.4625e+00,  1.6944e+00,\n",
       "           1.9612e+00,  2.4136e+00,  3.1492e+00,  4.5760e+00,  1.7874e+00,\n",
       "          -8.5457e-02, -3.0251e-02,  0.0000e+00],\n",
       "         [ 0.0000e+00, -1.7417e-02, -3.0538e-02, -7.5927e-02, -1.8656e-01,\n",
       "           3.9386e+00,  2.7525e+00,  2.2180e+00,  1.9688e+00,  1.8410e+00,\n",
       "           1.7540e+00,  1.6642e+00,  1.5147e+00,  1.3243e+00,  8.2705e-01,\n",
       "          -2.4598e-01, -7.6842e-01, -3.4750e-01,  9.0650e-01,  1.6790e+00,\n",
       "           1.9813e+00,  2.4724e+00,  3.3162e+00,  4.4008e+00,  1.6290e+00,\n",
       "          -8.2163e-02, -3.0250e-02, -1.2911e-02],\n",
       "         [ 0.0000e+00, -1.2911e-02, -3.9437e-02, -8.8885e-02, -2.0591e-01,\n",
       "           2.3629e+00,  2.6733e+00,  2.2351e+00,  2.0356e+00,  1.9089e+00,\n",
       "           1.7955e+00,  1.6867e+00,  1.6252e+00,  1.3198e+00,  7.6777e-02,\n",
       "           3.8612e-02,  2.3647e-01,  1.3597e+00,  1.4804e+00,  1.7008e+00,\n",
       "           2.0627e+00,  2.5852e+00,  3.5031e+00,  2.5783e+00, -1.5316e-01,\n",
       "          -8.3323e-02, -3.2104e-02, -1.2911e-02],\n",
       "         [ 0.0000e+00,  0.0000e+00, -4.5426e-02, -1.0123e-01, -2.1588e-01,\n",
       "           6.8404e-01,  2.6175e+00,  2.1804e+00,  1.9609e+00,  1.8025e+00,\n",
       "           1.6812e+00,  1.6018e+00,  1.5267e+00,  1.3890e+00,  1.2662e+00,\n",
       "           1.2334e+00,  1.2544e+00,  1.3159e+00,  1.4692e+00,  1.7585e+00,\n",
       "           2.1882e+00,  2.7867e+00,  3.1550e+00,  6.9339e-01, -1.3338e-01,\n",
       "          -6.9909e-02, -2.9398e-02, -1.2911e-02],\n",
       "         [ 0.0000e+00,  0.0000e+00, -4.5347e-02, -1.0857e-01, -2.1304e-01,\n",
       "           4.7965e-01,  2.2996e+00,  2.1001e+00,  1.8096e+00,  1.6245e+00,\n",
       "           1.5109e+00,  1.4232e+00,  1.3277e+00,  1.2120e+00,  1.1486e+00,\n",
       "           1.1419e+00,  1.2008e+00,  1.3397e+00,  1.5659e+00,  1.9277e+00,\n",
       "           2.1865e+00,  5.4128e-01, -1.5243e-01, -1.8532e-01, -1.2019e-01,\n",
       "          -5.8446e-02, -1.9477e-02,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00, -4.3288e-02, -1.0115e-01, -1.9124e-01,\n",
       "          -3.0293e-01,  4.7487e-01,  1.8691e+00,  1.7675e+00,  1.5205e+00,\n",
       "           1.3748e+00,  1.2580e+00,  1.1668e+00,  1.0956e+00,  1.0753e+00,\n",
       "           1.1316e+00,  1.2730e+00,  1.5168e+00,  1.8439e+00,  1.0309e+00,\n",
       "           5.8436e-01, -3.0194e-01, -2.2114e-01, -1.5545e-01, -1.0322e-01,\n",
       "          -5.0067e-02, -2.4689e-02,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00, -3.1459e-02, -7.7364e-02, -1.4857e-01,\n",
       "          -2.4497e-01, -3.6009e-01,  1.9794e-01,  1.9515e+00,  1.6133e+00,\n",
       "           1.3814e+00,  1.2201e+00,  1.1377e+00,  1.1065e+00,  1.1488e+00,\n",
       "           1.2832e+00,  1.2740e+00, -1.0105e-02,  2.1193e-01, -3.4384e-01,\n",
       "          -3.0795e-01, -2.2835e-01, -1.7103e-01, -1.1659e-01, -7.6317e-02,\n",
       "          -4.2900e-02, -1.4560e-02,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00, -2.5493e-02, -4.8041e-02, -9.7657e-02,\n",
       "          -1.6847e-01, -2.6266e-01, -3.7040e-01, -4.8904e-01, -6.1138e-01,\n",
       "          -7.4816e-01, -8.6120e-01, -9.1958e-01, -9.1647e-01, -8.5433e-01,\n",
       "          -7.4341e-01, -6.1391e-01, -4.9645e-01, -3.8424e-01, -2.9604e-01,\n",
       "          -2.1653e-01, -1.5834e-01, -1.1722e-01, -7.7016e-02, -5.3235e-02,\n",
       "          -2.1041e-02,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00, -1.7886e-02, -3.0319e-02, -5.5234e-02,\n",
       "          -8.8365e-02, -1.4924e-01, -2.2550e-01, -2.9916e-01, -3.8046e-01,\n",
       "          -4.6291e-01, -5.2782e-01, -5.6766e-01, -5.6009e-01, -5.2289e-01,\n",
       "          -4.6270e-01, -3.9230e-01, -3.2637e-01, -2.5595e-01, -1.9828e-01,\n",
       "          -1.4140e-01, -1.0343e-01, -7.5345e-02, -5.1741e-02, -3.1824e-02,\n",
       "          -1.8193e-02, -1.2911e-02,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.2911e-02, -2.9511e-02,\n",
       "          -4.9143e-02, -7.8366e-02, -1.1821e-01, -1.6896e-01, -2.1577e-01,\n",
       "          -2.6296e-01, -2.9467e-01, -3.0560e-01, -2.9955e-01, -2.8147e-01,\n",
       "          -2.5394e-01, -2.2454e-01, -1.9039e-01, -1.5630e-01, -1.2318e-01,\n",
       "          -8.8320e-02, -6.4422e-02, -4.9998e-02, -2.8616e-02, -2.3265e-02,\n",
       "          -1.2911e-02, -1.2911e-02,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4971e-02,\n",
       "          -3.0462e-02, -5.0573e-02, -7.6303e-02, -1.0492e-01, -1.3026e-01,\n",
       "          -1.5972e-01, -1.7478e-01, -1.8147e-01, -1.7495e-01, -1.5959e-01,\n",
       "          -1.3902e-01, -1.1854e-01, -1.0673e-01, -8.8302e-02, -7.8607e-02,\n",
       "          -5.5472e-02, -4.2591e-02, -2.4352e-02, -1.3191e-02,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00, -1.9345e-02, -3.4170e-02, -4.1353e-02,\n",
       "          -4.4095e-02, -4.2658e-02, -4.9949e-02, -5.6413e-02, -5.8557e-02,\n",
       "          -5.2041e-02, -5.4328e-02, -4.1349e-02, -2.6952e-02, -1.6352e-02,\n",
       "          -1.2911e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00]]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6000/6000 [00:00<00:00, 9022.03it/s]\n"
     ]
    }
   ],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "# MNIST samples don't have 3 channels so RGB doesnt work for them\n",
    "  transforms.ToPILImage(),\n",
    "  transforms.ToTensor(),\n",
    "#   transforms.Normalize(\n",
    "#       (0.4914, 0.4822, 0.4465),\n",
    "#       (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "cont_dataset2 = MNIST_Cont_Dataset(df_std, transform=train_transforms, is_test = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.9176e-02,\n",
       "          -2.5940e-02, -2.5779e-02, -3.1474e-02, -2.6694e-02, -3.3916e-02,\n",
       "          -2.8713e-02, -2.6726e-02, -2.8346e-02, -2.3016e-02, -1.6706e-02,\n",
       "          -1.2911e-02, -1.2911e-02, -1.2911e-02, -1.2911e-02,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.2911e-02,\n",
       "          -1.2911e-02,  0.0000e+00, -1.2911e-02, -1.3903e-02, -4.4102e-02,\n",
       "          -6.3960e-02, -7.8035e-02, -9.8120e-02, -1.1828e-01, -1.3359e-01,\n",
       "          -1.4207e-01, -1.3674e-01, -1.2964e-01, -1.1159e-01, -9.0272e-02,\n",
       "          -7.1527e-02, -5.8973e-02, -4.6033e-02, -2.2656e-02, -1.2911e-02,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.2911e-02, -1.7241e-02,\n",
       "          -2.4673e-02, -3.1852e-02, -4.6284e-02, -6.8639e-02, -1.0393e-01,\n",
       "          -1.3221e-01, -1.6399e-01, -1.9143e-01, -2.1645e-01, -2.4199e-01,\n",
       "          -2.5219e-01, -2.5393e-01,  3.6940e-01,  1.4283e+00,  6.6817e+00,\n",
       "           9.0774e+00,  1.1317e+01,  1.3687e+01, -4.2039e-02, -2.1931e-02,\n",
       "          -1.2911e-02,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.5507e-02, -2.0172e-02,\n",
       "          -5.3984e-02, -8.0049e-02, -1.2224e-01, -1.6791e-01, -2.1339e-01,\n",
       "          -2.6985e-01, -3.2996e-01, -3.9506e-01, -4.5540e-01, -5.0399e-01,\n",
       "          -5.3213e-01,  8.5071e-01,  1.7417e+00,  3.0501e+00,  3.6314e+00,\n",
       "           4.4952e+00,  6.0826e+00,  6.9691e+00, -9.5593e-02, -6.6610e-02,\n",
       "          -3.9739e-02, -1.7659e-02, -1.2911e-02],\n",
       "         [ 0.0000e+00,  0.0000e+00, -1.8106e-02, -2.9180e-02, -5.8009e-02,\n",
       "          -9.7757e-02, -1.4923e-01, -2.0462e-01, -2.6563e-01, -3.4022e-01,\n",
       "          -4.2883e-01, -5.2468e-01, -5.6512e-01, -6.3979e-01, -8.4430e-02,\n",
       "           9.3693e-01,  1.4241e+00,  1.6220e+00,  1.8948e+00,  2.2802e+00,\n",
       "           2.8419e+00,  3.7738e+00,  5.0103e+00,  2.8808e+00, -1.2742e-01,\n",
       "          -7.5766e-02, -3.1775e-02, -1.2911e-02],\n",
       "         [ 0.0000e+00, -1.2911e-02, -1.5586e-02, -3.9114e-02, -8.1819e-02,\n",
       "          -1.4535e-01, -2.1041e-01, -2.8187e-01, -3.6459e-01, -4.6201e-01,\n",
       "          -5.7282e-01, -4.4653e-01,  8.2407e-01,  1.2786e+00,  1.1486e+00,\n",
       "           1.1011e+00,  1.1370e+00,  1.2383e+00,  1.3997e+00,  1.6845e+00,\n",
       "           2.1043e+00,  2.8057e+00,  2.9427e+00, -2.3725e-01, -1.6588e-01,\n",
       "          -1.0277e-01, -5.2271e-02, -1.2911e-02],\n",
       "         [ 0.0000e+00, -1.6307e-02, -3.1119e-02, -6.2291e-02, -1.1818e-01,\n",
       "          -1.8503e-01, -2.6077e-01, -3.5161e-01, -4.5071e-01, -5.8068e-01,\n",
       "           4.2905e-01,  7.8460e-01,  1.2698e+00,  1.1406e+00,  1.0813e+00,\n",
       "           1.0605e+00,  1.0948e+00,  1.1561e+00,  1.2550e+00,  1.4493e+00,\n",
       "           1.7853e+00,  2.3539e+00,  9.1781e-01, -2.8111e-01, -1.9153e-01,\n",
       "          -1.1294e-01, -5.5939e-02, -1.8225e-02],\n",
       "         [-1.2911e-02, -2.1510e-02, -5.0715e-02, -8.4582e-02, -1.4387e-01,\n",
       "          -2.1651e-01, -3.0432e-01, -3.9706e-01, -3.1383e-01,  5.6833e-01,\n",
       "           1.4421e+00,  1.3010e+00,  1.1827e+00,  1.1645e+00,  1.1684e+00,\n",
       "           1.1855e+00,  1.2046e+00,  1.1975e+00,  1.1170e+00, -3.7067e-01,\n",
       "          -3.0130e-01, -7.4369e-02, -3.9401e-01, -2.9383e-01, -1.9305e-01,\n",
       "          -1.0922e-01, -5.0680e-02, -1.7607e-02],\n",
       "         [ 0.0000e+00, -1.8840e-02, -5.7865e-02, -9.2093e-02, -1.5650e-01,\n",
       "          -2.2960e-01, -3.2744e-01, -3.1009e-01,  2.1635e+00,  1.7085e+00,\n",
       "           1.4150e+00,  1.2694e+00,  1.2611e+00,  1.3558e+00,  1.4175e+00,\n",
       "           1.4425e+00,  7.0051e-01,  5.4603e-01, -4.7612e-01, -8.8265e-01,\n",
       "          -7.2724e-01, -5.6329e-01, -4.2046e-01, -2.9180e-01, -1.8350e-01,\n",
       "          -9.9339e-02, -4.4173e-02,  0.0000e+00],\n",
       "         [ 0.0000e+00, -2.2130e-02, -5.1649e-02, -9.2287e-02, -1.4960e-01,\n",
       "          -2.2982e-01, -3.2921e-01, -3.2453e-01,  2.0645e+00,  1.6412e+00,\n",
       "           1.3944e+00,  1.3435e+00,  1.4334e+00,  1.6021e+00,  1.1212e+00,\n",
       "           1.9948e-01, -7.4886e-01, -9.2543e-01, -9.3095e-01, -8.3793e-01,\n",
       "          -6.9022e-01, -5.3961e-01, -3.9855e-01, -2.7472e-01, -1.6428e-01,\n",
       "          -9.1204e-02, -3.6137e-02,  0.0000e+00],\n",
       "         [ 0.0000e+00, -2.4455e-02, -4.6926e-02, -8.7883e-02, -1.4133e-01,\n",
       "          -4.1857e-02,  1.7048e+00,  2.1952e+00,  1.9473e+00,  1.5561e+00,\n",
       "           1.3809e+00,  1.4134e+00,  1.4362e+00,  1.2476e+00, -5.2373e-01,\n",
       "          -7.9794e-01, -8.9809e-01, -9.5863e-01, -9.2944e-01, -5.5349e-01,\n",
       "          -4.8205e-01, -5.0142e-01, -3.7521e-01, -2.6297e-01, -1.5585e-01,\n",
       "          -6.5685e-02, -2.7645e-02, -1.2911e-02],\n",
       "         [ 0.0000e+00, -2.0389e-02, -3.2903e-02, -7.5266e-02, -1.2842e-01,\n",
       "           1.3326e+00,  3.5393e+00,  2.4599e+00,  1.8358e+00,  1.4991e+00,\n",
       "           1.3978e+00,  1.1782e+00, -1.6401e-01, -6.6933e-01,  4.2712e-02,\n",
       "          -6.0146e-02, -1.6065e-01, -1.7987e-01, -8.1510e-02,  1.5189e+00,\n",
       "           1.3072e+00,  6.7906e-01,  7.2184e-01, -2.5527e-01, -1.5458e-01,\n",
       "          -5.7072e-02, -1.8564e-02, -1.2911e-02],\n",
       "         [ 0.0000e+00, -1.5894e-02, -3.2841e-02, -5.9466e-02, -1.2713e-01,\n",
       "           1.2638e+00,  3.3697e+00,  2.3176e+00,  1.7596e+00,  1.4967e+00,\n",
       "           1.3146e+00, -2.7604e-01,  8.3390e-02,  6.7397e-01,  1.2385e+00,\n",
       "           1.1641e+00,  1.1285e+00,  1.1447e+00,  1.3119e+00,  1.6701e+00,\n",
       "           2.1176e+00,  2.5940e+00,  2.9966e+00,  1.3614e+00, -1.5745e-01,\n",
       "          -6.4377e-02, -2.4703e-02,  0.0000e+00],\n",
       "         [ 0.0000e+00, -1.5405e-02, -2.6326e-02, -4.7872e-02, -1.3496e-01,\n",
       "           4.5913e+00,  3.1237e+00,  2.2125e+00,  1.7307e+00,  1.5031e+00,\n",
       "           1.4327e+00,  1.2184e+00,  1.3259e+00,  1.2295e+00,  1.0801e+00,\n",
       "           1.0316e+00,  1.0589e+00,  1.1218e+00,  1.3264e+00,  1.6685e+00,\n",
       "           2.0624e+00,  2.4581e+00,  3.1455e+00,  4.1743e+00,  1.5777e+00,\n",
       "          -7.5762e-02, -1.9122e-02,  0.0000e+00],\n",
       "         [-1.2911e-02, -1.2911e-02, -2.2975e-02, -5.3478e-02, -1.4668e-01,\n",
       "           4.4801e+00,  2.8958e+00,  2.1646e+00,  1.7673e+00,  1.5744e+00,\n",
       "           1.5183e+00,  1.4747e+00,  1.3512e+00,  1.1546e+00,  1.0425e+00,\n",
       "           1.0336e+00,  1.0845e+00,  1.1640e+00,  1.3860e+00,  1.6715e+00,\n",
       "           2.0163e+00,  2.4081e+00,  3.1000e+00,  4.4491e+00,  1.7715e+00,\n",
       "          -8.3040e-02, -2.8194e-02,  0.0000e+00],\n",
       "         [-1.2911e-02, -1.2911e-02, -2.1765e-02, -6.0894e-02, -1.6452e-01,\n",
       "           4.1336e+00,  2.7922e+00,  2.1822e+00,  1.8401e+00,  1.6751e+00,\n",
       "           1.6117e+00,  1.5415e+00,  1.3836e+00,  1.2157e+00,  1.1233e+00,\n",
       "           1.1268e+00,  1.1829e+00,  1.2769e+00,  1.4625e+00,  1.6944e+00,\n",
       "           1.9612e+00,  2.4136e+00,  3.1492e+00,  4.5760e+00,  1.7874e+00,\n",
       "          -8.5457e-02, -3.0251e-02,  0.0000e+00],\n",
       "         [ 0.0000e+00, -1.7417e-02, -3.0538e-02, -7.5927e-02, -1.8656e-01,\n",
       "           3.9386e+00,  2.7525e+00,  2.2180e+00,  1.9688e+00,  1.8410e+00,\n",
       "           1.7540e+00,  1.6642e+00,  1.5147e+00,  1.3243e+00,  8.2705e-01,\n",
       "          -2.4598e-01, -7.6842e-01, -3.4750e-01,  9.0650e-01,  1.6790e+00,\n",
       "           1.9813e+00,  2.4724e+00,  3.3162e+00,  4.4008e+00,  1.6290e+00,\n",
       "          -8.2163e-02, -3.0250e-02, -1.2911e-02],\n",
       "         [ 0.0000e+00, -1.2911e-02, -3.9437e-02, -8.8885e-02, -2.0591e-01,\n",
       "           2.3629e+00,  2.6733e+00,  2.2351e+00,  2.0356e+00,  1.9089e+00,\n",
       "           1.7955e+00,  1.6867e+00,  1.6252e+00,  1.3198e+00,  7.6777e-02,\n",
       "           3.8612e-02,  2.3647e-01,  1.3597e+00,  1.4804e+00,  1.7008e+00,\n",
       "           2.0627e+00,  2.5852e+00,  3.5031e+00,  2.5783e+00, -1.5316e-01,\n",
       "          -8.3323e-02, -3.2104e-02, -1.2911e-02],\n",
       "         [ 0.0000e+00,  0.0000e+00, -4.5426e-02, -1.0123e-01, -2.1588e-01,\n",
       "           6.8404e-01,  2.6175e+00,  2.1804e+00,  1.9609e+00,  1.8025e+00,\n",
       "           1.6812e+00,  1.6018e+00,  1.5267e+00,  1.3890e+00,  1.2662e+00,\n",
       "           1.2334e+00,  1.2544e+00,  1.3159e+00,  1.4692e+00,  1.7585e+00,\n",
       "           2.1882e+00,  2.7867e+00,  3.1550e+00,  6.9339e-01, -1.3338e-01,\n",
       "          -6.9909e-02, -2.9398e-02, -1.2911e-02],\n",
       "         [ 0.0000e+00,  0.0000e+00, -4.5347e-02, -1.0857e-01, -2.1304e-01,\n",
       "           4.7965e-01,  2.2996e+00,  2.1001e+00,  1.8096e+00,  1.6245e+00,\n",
       "           1.5109e+00,  1.4232e+00,  1.3277e+00,  1.2120e+00,  1.1486e+00,\n",
       "           1.1419e+00,  1.2008e+00,  1.3397e+00,  1.5659e+00,  1.9277e+00,\n",
       "           2.1865e+00,  5.4128e-01, -1.5243e-01, -1.8532e-01, -1.2019e-01,\n",
       "          -5.8446e-02, -1.9477e-02,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00, -4.3288e-02, -1.0115e-01, -1.9124e-01,\n",
       "          -3.0293e-01,  4.7487e-01,  1.8691e+00,  1.7675e+00,  1.5205e+00,\n",
       "           1.3748e+00,  1.2580e+00,  1.1668e+00,  1.0956e+00,  1.0753e+00,\n",
       "           1.1316e+00,  1.2730e+00,  1.5168e+00,  1.8439e+00,  1.0309e+00,\n",
       "           5.8436e-01, -3.0194e-01, -2.2114e-01, -1.5545e-01, -1.0322e-01,\n",
       "          -5.0067e-02, -2.4689e-02,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00, -3.1459e-02, -7.7364e-02, -1.4857e-01,\n",
       "          -2.4497e-01, -3.6009e-01,  1.9794e-01,  1.9515e+00,  1.6133e+00,\n",
       "           1.3814e+00,  1.2201e+00,  1.1377e+00,  1.1065e+00,  1.1488e+00,\n",
       "           1.2832e+00,  1.2740e+00, -1.0105e-02,  2.1193e-01, -3.4384e-01,\n",
       "          -3.0795e-01, -2.2835e-01, -1.7103e-01, -1.1659e-01, -7.6317e-02,\n",
       "          -4.2900e-02, -1.4560e-02,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00, -2.5493e-02, -4.8041e-02, -9.7657e-02,\n",
       "          -1.6847e-01, -2.6266e-01, -3.7040e-01, -4.8904e-01, -6.1138e-01,\n",
       "          -7.4816e-01, -8.6120e-01, -9.1958e-01, -9.1647e-01, -8.5433e-01,\n",
       "          -7.4341e-01, -6.1391e-01, -4.9645e-01, -3.8424e-01, -2.9604e-01,\n",
       "          -2.1653e-01, -1.5834e-01, -1.1722e-01, -7.7016e-02, -5.3235e-02,\n",
       "          -2.1041e-02,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00, -1.7886e-02, -3.0319e-02, -5.5234e-02,\n",
       "          -8.8365e-02, -1.4924e-01, -2.2550e-01, -2.9916e-01, -3.8046e-01,\n",
       "          -4.6291e-01, -5.2782e-01, -5.6766e-01, -5.6009e-01, -5.2289e-01,\n",
       "          -4.6270e-01, -3.9230e-01, -3.2637e-01, -2.5595e-01, -1.9828e-01,\n",
       "          -1.4140e-01, -1.0343e-01, -7.5345e-02, -5.1741e-02, -3.1824e-02,\n",
       "          -1.8193e-02, -1.2911e-02,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.2911e-02, -2.9511e-02,\n",
       "          -4.9143e-02, -7.8366e-02, -1.1821e-01, -1.6896e-01, -2.1577e-01,\n",
       "          -2.6296e-01, -2.9467e-01, -3.0560e-01, -2.9955e-01, -2.8147e-01,\n",
       "          -2.5394e-01, -2.2454e-01, -1.9039e-01, -1.5630e-01, -1.2318e-01,\n",
       "          -8.8320e-02, -6.4422e-02, -4.9998e-02, -2.8616e-02, -2.3265e-02,\n",
       "          -1.2911e-02, -1.2911e-02,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4971e-02,\n",
       "          -3.0462e-02, -5.0573e-02, -7.6303e-02, -1.0492e-01, -1.3026e-01,\n",
       "          -1.5972e-01, -1.7478e-01, -1.8147e-01, -1.7495e-01, -1.5959e-01,\n",
       "          -1.3902e-01, -1.1854e-01, -1.0673e-01, -8.8302e-02, -7.8607e-02,\n",
       "          -5.5472e-02, -4.2591e-02, -2.4352e-02, -1.3191e-02,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00, -1.9345e-02, -3.4170e-02, -4.1353e-02,\n",
       "          -4.4095e-02, -4.2658e-02, -4.9949e-02, -5.6413e-02, -5.8557e-02,\n",
       "          -5.2041e-02, -5.4328e-02, -4.1349e-02, -2.6952e-02, -1.6352e-02,\n",
       "          -1.2911e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00]]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_dataset2[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
